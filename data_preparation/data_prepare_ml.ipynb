{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f80ba075-e10d-458a-b00d-65a6957afa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 1605\n",
      "Missing: 0\n"
     ]
    }
   ],
   "source": [
    "# statistical imputation transform for the horse colic dataset\n",
    "from numpy import isnan\n",
    "from pandas import read_csv\n",
    "from sklearn.impute import SimpleImputer\n",
    "# load dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv'\n",
    "dataframe = read_csv(url, header=None, na_values='?')\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(X).flatten()))\n",
    "# define imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "# fit on the dataset\n",
    "imputer.fit(X)\n",
    "# transform the dataset\n",
    "Xtrans = imputer.transform(X)\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(Xtrans).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "680d54d5-6f65-47c1-b6ec-6167f7e7111f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 0, Selected=False, Rank: 5\n",
      "Column: 1, Selected=False, Rank: 4\n",
      "Column: 2, Selected=True, Rank: 1\n",
      "Column: 3, Selected=True, Rank: 1\n",
      "Column: 4, Selected=True, Rank: 1\n",
      "Column: 5, Selected=False, Rank: 6\n",
      "Column: 6, Selected=True, Rank: 1\n",
      "Column: 7, Selected=False, Rank: 2\n",
      "Column: 8, Selected=True, Rank: 1\n",
      "Column: 9, Selected=False, Rank: 3\n"
     ]
    }
   ],
   "source": [
    "# report which features were selected by RFE\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "# define RFE\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\n",
    "# fit RFE\n",
    "rfe.fit(X, y)\n",
    "# summarize all features\n",
    "for i in range(X.shape[1]):\n",
    "\tprint('Column: %d, Selected=%s, Rank: %d' % (i, rfe.support_[i], rfe.ranking_[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74772a41-7192-437c-a17f-2127e3ce156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.39324489 -5.77732048 -0.59062319 -2.08095322  1.04707034]\n",
      " [-0.45820294  1.94683482 -2.46471441  2.36590955 -0.73666725]\n",
      " [ 2.35162422 -1.00061698 -0.5946091   1.12531096 -0.65267587]]\n",
      "[[0.77608466 0.0239289  0.48251588 0.18352101 0.59830036]\n",
      " [0.40400165 0.79590304 0.27369632 0.6331332  0.42104156]\n",
      " [0.77065362 0.50132629 0.48207176 0.5076991  0.4293882 ]]\n"
     ]
    }
   ],
   "source": [
    "# example of normalizing input data\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=5, n_informative=5, n_redundant=0, random_state=1)\n",
    "# summarize data before the transform\n",
    "print(X[:3, :])\n",
    "# define the scaler\n",
    "trans = MinMaxScaler()\n",
    "# transform the data\n",
    "X_norm = trans.fit_transform(X)\n",
    "# summarize data after the transform\n",
    "print(X_norm[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "156889b2-056e-4d6c-937a-34339789a682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"'40-49'\" \"'premeno'\" \"'15-19'\" \"'0-2'\" \"'yes'\" \"'3'\" \"'right'\"\n",
      "  \"'left_up'\" \"'no'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'15-19'\" \"'0-2'\" \"'no'\" \"'1'\" \"'right'\" \"'central'\"\n",
      "  \"'no'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'35-39'\" \"'0-2'\" \"'no'\" \"'2'\" \"'left'\" \"'left_low'\"\n",
      "  \"'no'\"]]\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode the breast cancer dataset\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# define the location of the dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv\"\n",
    "# load the dataset\n",
    "dataset = read_csv(url, header=None)\n",
    "# retrieve the array of data\n",
    "data = dataset.values\n",
    "# separate into input and output columns\n",
    "X = data[:, :-1].astype(str)\n",
    "y = data[:, -1].astype(str)\n",
    "# summarize the raw data\n",
    "print(X[:3, :])\n",
    "# define the one hot encoding transform\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "# fit and apply the transform to the input data\n",
    "X_oe = encoder.fit_transform(X)\n",
    "# summarize the transformed data\n",
    "print(X_oe[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae762a2b-b26f-470b-9c33-4b42ea10b486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.39324489 -5.77732048 -0.59062319 -2.08095322  1.04707034]\n",
      " [-0.45820294  1.94683482 -2.46471441  2.36590955 -0.73666725]\n",
      " [ 2.35162422 -1.00061698 -0.5946091   1.12531096 -0.65267587]]\n",
      "[[7. 0. 4. 1. 5.]\n",
      " [4. 7. 2. 6. 4.]\n",
      " [7. 5. 4. 5. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# discretize numeric input variables\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=5, n_informative=5, n_redundant=0, random_state=1)\n",
    "# summarize data before the transform\n",
    "print(X[:3, :])\n",
    "# define the transform\n",
    "trans = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "# transform the data\n",
    "X_discrete = trans.fit_transform(X)\n",
    "# summarize data after the transform\n",
    "print(X_discrete[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e937821b-4c7e-4896-9a68-8c2598ba7b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.53448246  0.93837451  0.38969914  0.0926655   1.70876508  1.14351305\n",
      "  -1.47034214  0.11857673 -2.72241741  0.2953565 ]\n",
      " [-2.42280473 -1.02658758 -2.34792156 -0.82422408  0.59933419 -2.44832253\n",
      "   0.39750207  2.0265065   1.83374105  0.72430365]\n",
      " [-1.83391794 -1.1946668  -0.73806871  1.50947233  1.78047734  0.58779205\n",
      "  -2.78506977 -0.04163788 -1.25227833  0.99373587]]\n",
      "[[-1.64710578  2.11683302 -1.98256096]\n",
      " [ 0.92840209 -4.8294997  -0.22727043]\n",
      " [-3.83677757 -0.32300714 -0.11512801]]\n"
     ]
    }
   ],
   "source": [
    "# example of pca for dimensionality reduction\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=3, n_redundant=7, random_state=1)\n",
    "# summarize data before the transform\n",
    "print(X[:3, :])\n",
    "# define the transform\n",
    "trans = PCA(n_components=3)\n",
    "# transform the data\n",
    "X_dim = trans.fit_transform(X)\n",
    "# summarize data after the transform\n",
    "print(X_dim[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a276e50-f3cf-45ae-902d-b114f06b52ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
