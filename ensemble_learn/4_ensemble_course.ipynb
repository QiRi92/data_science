{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b283f4-8457-4e03-957c-e9c338d6470a",
   "metadata": {},
   "source": [
    "# Ensemble Machine Learning With Python (7-Day Mini-Course)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73d8814-13be-461a-a067-c0fcc25feb51",
   "metadata": {},
   "source": [
    "Ensemble learning refers to machine learning models that combine the predictions from two or more models.\n",
    "\n",
    "Ensembles are an advanced approach to machine learning that are often used when the capability and skill of the predictions are more important than using a simple and understandable model. As such, they are often used by top and winning participants in machine learning competitions like the [One Million Dollar Netflix Prize](https://en.wikipedia.org/wiki/Netflix_Prize) and [Kaggle Competitions](https://www.kaggle.com/).\n",
    "\n",
    "Modern machine learning libraries like scikit-learn Python provide a suite of advanced ensemble learning methods that are easy to configure and use correctly without data leakage, a common concern when using ensemble algorithms.\n",
    "\n",
    "In this crash course, you will discover how you can get started and confidently bring ensemble learning algorithms to your predictive modeling project with Python in seven days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13659ba-95ac-4827-8643-eda382a27cba",
   "metadata": {},
   "source": [
    "## Crash-Course Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5584628-5dd3-441c-9c9c-3e748d0f67c4",
   "metadata": {},
   "source": [
    "This crash course is broken down into seven lessons.\n",
    "\n",
    "You could complete one lesson per day (recommended) or complete all of the lessons in one day (hardcore). It really depends on the time you have available and your level of enthusiasm.\n",
    "\n",
    "Below is a list of the seven lessons that will get you started and productive with data preparation in Python:\n",
    "\n",
    "- **Lesson 01**: What Is Ensemble Learning?\n",
    "- **Lesson 02**: Bagging Ensembles\n",
    "- **Lesson 03**: Random Forest Ensemble\n",
    "- **Lesson 04**: AdaBoost Ensemble\n",
    "- **Lesson 05**: Gradient Boosting Ensemble\n",
    "- **Lesson 06**: Voting Ensemble\n",
    "- **Lesson 07**: Stacking Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2556b77b-48a9-4df5-b7bc-c1d9989bfe99",
   "metadata": {},
   "source": [
    "## Lesson 01: What Is Ensemble Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ff90d6-042d-4b1a-8350-b44da0b37eb0",
   "metadata": {},
   "source": [
    "In this lesson, you will discover what ensemble learning is and why it is important.\n",
    "\n",
    "Applied machine learning often involves fitting and evaluating models on a dataset.\n",
    "\n",
    "Given that we cannot know which model will perform best on the dataset beforehand, this may involve a lot of trial and error until we find a model that performs well or best for our project.\n",
    "\n",
    "An alternate approach is to prepare multiple different models, then combine their predictions.\n",
    "\n",
    "This is called an ensemble machine learning model, or simply an ensemble, and the process of finding a well-performing ensemble model is referred to as “*ensemble learning*.”\n",
    "\n",
    "Although there is nearly an unlimited number of ways that this can be achieved, there are perhaps three classes of ensemble learning techniques that are most commonly discussed and used in practice.\n",
    "\n",
    "Their popularity is due in large part to their ease of implementation and success on a wide range of predictive modeling problems.\n",
    "\n",
    "They are:\n",
    "\n",
    "- **Bagging**, e.g. bagged decision trees and random forest.\n",
    "- **Boosting**, e.g. adaboost and gradient boosting\n",
    "- **Stacking**, e.g. voting and using a meta-model.\n",
    "\n",
    "There are two main reasons to use an ensemble over a single model, and they are related; they are:\n",
    "\n",
    "- **Reliability**: Ensembles can reduce the variance of the predictions.\n",
    "- **Skill**: Ensembles can achieve better performance than a single model.\n",
    "\n",
    "These are both important concerns on a machine learning project and sometimes we may prefer one or both properties from a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acda2de0-79f7-418a-92eb-8e758337cacc",
   "metadata": {},
   "source": [
    "## Lesson 02: Bagging Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c771fd05-11c2-40b7-832c-737570880653",
   "metadata": {},
   "source": [
    "In this lesson, you will discover the **bootstrap aggregation**, or bagging, ensemble.\n",
    "\n",
    "Bagging works by creating samples of the training dataset and fitting a decision tree on each sample.\n",
    "\n",
    "The differences in the training datasets result in differences in the fit decision trees, and in turn, differences in predictions made by those trees. The predictions made by the ensemble members are then combined using simple statistics, such as voting or averaging.\n",
    "\n",
    "Key to the method is the manner in which each sample of the dataset is prepared to train ensemble members. Examples (rows) are drawn from the dataset at random, although with replacement. Replacement means that if a row is selected, it is returned to the training dataset for potential re-selection in the same training dataset.\n",
    "\n",
    "This is called a bootstrap sample, giving the technique its name.\n",
    "\n",
    "Bagging is available in scikit-learn via the [BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) and [BaggingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html) classes, which use a decision tree as the base-model by default and you can specify the number of trees to create via the “*n_estimators*” argument.\n",
    "\n",
    "The complete example of evaluating a bagging ensemble for classification is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6325745-41f4-494b-a07d-a5944bb1c8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.947 (0.072)\n"
     ]
    }
   ],
   "source": [
    "# No change\n",
    "# example of evaluating a bagging ensemble for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# create the synthetic classification dataset\n",
    "X, y = make_classification(random_state=1)\n",
    "# configure the ensemble model\n",
    "model = BaggingClassifier(n_estimators=50)\n",
    "# configure the resampling method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the ensemble on the dataset using the resampling method\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report ensemble performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c203fb74-834b-4f7d-a3e1-3afe4d2cc359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.953 (0.067)\n"
     ]
    }
   ],
   "source": [
    "# Increase n_estimators=100\n",
    "# example of evaluating a bagging ensemble for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# create the synthetic classification dataset\n",
    "X, y = make_classification(random_state=1)\n",
    "# configure the ensemble model\n",
    "model = BaggingClassifier(n_estimators=100)\n",
    "# configure the resampling method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the ensemble on the dataset using the resampling method\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report ensemble performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "724df24b-de4e-43d7-87d2-9db954f7ab2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.950 (0.067)\n"
     ]
    }
   ],
   "source": [
    "# decrease n_estimators=25\n",
    "# example of evaluating a bagging ensemble for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# create the synthetic classification dataset\n",
    "X, y = make_classification(random_state=1)\n",
    "# configure the ensemble model\n",
    "model = BaggingClassifier(n_estimators=25)\n",
    "# configure the resampling method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the ensemble on the dataset using the resampling method\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report ensemble performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5512a26-cd20-4702-9926-88c4df8753dd",
   "metadata": {},
   "source": [
    "## Lesson 03: Random Forest Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f84b9-abdb-4388-bee7-df7c65dc519f",
   "metadata": {},
   "source": [
    "In this lesson, you will discover the random forest ensemble.\n",
    "\n",
    "Random forest is an extension of the bagging ensemble.\n",
    "\n",
    "Like bagging, the random forest ensemble fits a decision tree on different bootstrap samples of the training dataset.\n",
    "\n",
    "Unlike bagging, random forest will also sample the features (columns) of each dataset.\n",
    "\n",
    "Specifically, split points are chosen in the data while constructing each decision tree. Rather than considering all features when choosing a split point, random forest limits the features to a random subset of features, such as 3 if there were 10 features.\n",
    "\n",
    "The random forest ensemble is available in scikit-learn via the [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) classes. You can specify the number of trees to create via the “*n_estimators*” argument and the number of randomly selected features to consider at each split point via the “*max_features*” argument, which is set to the square root of the number of features in your dataset by default.\n",
    "\n",
    "The complete example of evaluating a random forest ensemble for classification is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87668c24-1c95-4062-81b0-d16995d9f654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.957 (0.067)\n"
     ]
    }
   ],
   "source": [
    "# example of evaluating a random forest ensemble for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# create the synthetic classification dataset\n",
    "X, y = make_classification(random_state=1)\n",
    "# configure the ensemble model\n",
    "model = RandomForestClassifier(n_estimators=50)\n",
    "# configure the resampling method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the ensemble on the dataset using the resampling method\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report ensemble performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20e270e2-88c9-4d5d-8e6a-1c98c2cef4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.960 (0.066)\n"
     ]
    }
   ],
   "source": [
    "# increase n_estimators=100 \n",
    "# example of evaluating a random forest ensemble for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# create the synthetic classification dataset\n",
    "X, y = make_classification(random_state=1)\n",
    "# configure the ensemble model\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "# configure the resampling method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the ensemble on the dataset using the resampling method\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report ensemble performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d2d0ef-4065-450b-ad18-b15fc9358bbf",
   "metadata": {},
   "source": [
    "## Lesson 04: AdaBoost Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b59807d-06a2-46a1-8081-7e9e99d1b353",
   "metadata": {},
   "source": [
    "In this lesson, you will discover the adaptive boosting or AdaBoost ensemble.\n",
    "\n",
    "Boosting involves adding models sequentially to the ensemble where new models attempt to correct the errors made by prior models already added to the ensemble. As such, the more ensemble members that are added, the fewer errors the ensemble is expected to make, at least to a limit supported by the data and before overfitting the training dataset.\n",
    "\n",
    "The idea of boosting was first developed as a theoretical idea, and the AdaBoost algorithm was the first successful approach to realizing a boosting-based ensemble algorithm.\n",
    "\n",
    "AdaBoost works by fitting decision trees on versions of the training dataset weighted so that the tree pays more attention to examples (rows) that the prior members got wrong, and less attention to those that the prior models got correct.\n",
    "\n",
    "Rather than full decision trees, AdaBoost uses very simple trees that make a single decision on one input variable before making a prediction. These short trees are referred to as decision stumps.\n",
    "\n",
    "AdaBoost is available in scikit-learn via the [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) and [AdaBoostRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html) classes, which use a decision tree (decision stump) as the base-model by default and you can specify the number of trees to create via the “*n_estimators*” argument.\n",
    "\n",
    "The complete example of evaluating an AdaBoost ensemble for classification is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44fea88-bb65-4f3b-96c9-2fbcc440157b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.947 (0.088)\n"
     ]
    }
   ],
   "source": [
    "# example of evaluating an adaboost ensemble for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# create the synthetic classification dataset\n",
    "X, y = make_classification(random_state=1)\n",
    "# configure the ensemble model\n",
    "model = AdaBoostClassifier(n_estimators=50)\n",
    "# configure the resampling method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the ensemble on the dataset using the resampling method\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report ensemble performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c0b6fbb-20ad-4561-97c8-810726b85094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.950 (0.089)\n"
     ]
    }
   ],
   "source": [
    "# Increase n_estimators=100\n",
    "# example of evaluating an adaboost ensemble for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# create the synthetic classification dataset\n",
    "X, y = make_classification(random_state=1)\n",
    "# configure the ensemble model\n",
    "model = AdaBoostClassifier(n_estimators=100)\n",
    "# configure the resampling method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the ensemble on the dataset using the resampling method\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report ensemble performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af994b1-ffbb-4a6b-b796-a9ff4ebfecdd",
   "metadata": {},
   "source": [
    "## Lesson 05: Gradient Boosting Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee65a2d-e7c1-4708-a441-7b8fb88ec1ef",
   "metadata": {},
   "source": [
    "In this lesson, you will discover the gradient boosting ensemble.\n",
    "\n",
    "Gradient boosting is a framework for boosting ensemble algorithms and an extension to AdaBoost.\n",
    "\n",
    "It re-frames boosting as an additive model under a statistical framework and allows for the use of arbitrary loss functions to make it more flexible and loss penalties (shrinkage) to reduce overfitting.\n",
    "\n",
    "Gradient boosting also introduces ideas of bagging to the ensemble members, such as sampling of the training dataset rows and columns, referred to as stochastic gradient boosting.\n",
    "\n",
    "It is a very successful ensemble technique for structured or tabular data, although it can be slow to fit a model given that models are added sequentially. More efficient implementations have been developed, such as the popular extreme gradient boosting (XGBoost) and light gradient boosting machines (LightGBM).\n",
    "\n",
    "Gradient boosting is available in scikit-learn via the [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) and [GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) classes, which use a decision tree as the base-model by default. You can specify the number of trees to create via the “*n_estimators*” argument and the learning rate that controls the contribution from each tree via the “*learning_rate*” argument that defaults to 0.1.\n",
    "\n",
    "The complete example of evaluating a gradient boosting ensemble for classification is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4faf9d-e982-4598-9a77-9cb91e6c41a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.927 (0.100)\n"
     ]
    }
   ],
   "source": [
    "# example of evaluating a gradient boosting ensemble for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# create the synthetic classification dataset\n",
    "X, y = make_classification(random_state=1)\n",
    "# configure the ensemble model\n",
    "model = GradientBoostingClassifier(n_estimators=50)\n",
    "# configure the resampling method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the ensemble on the dataset using the resampling method\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report ensemble performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8775a78f-e73f-48e7-8a25-1030f6b34697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.923 (0.099)\n"
     ]
    }
   ],
   "source": [
    "# Increase n_estimators=100\n",
    "# example of evaluating a gradient boosting ensemble for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# create the synthetic classification dataset\n",
    "X, y = make_classification(random_state=1)\n",
    "# configure the ensemble model\n",
    "model = GradientBoostingClassifier(n_estimators=100)\n",
    "# configure the resampling method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the ensemble on the dataset using the resampling method\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report ensemble performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee85a41-20c4-4ae4-bd51-7e88668d61d1",
   "metadata": {},
   "source": [
    "## Lesson 06: Voting Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ea1702-e69c-44d6-bd50-6e8376ce8714",
   "metadata": {},
   "source": [
    "In this lesson, you will discover the voting ensemble.\n",
    "\n",
    "Voting ensembles use simple statistics to combine the predictions from multiple models.\n",
    "\n",
    "Typically, this involves fitting multiple different model types on the same training dataset, then calculating the average prediction in the case of regression or the class label with the most votes for classification, called hard voting.\n",
    "\n",
    "Voting can also be used when predicting the probability of class labels on classification problems by summing predicted probabilities and selecting the label with the largest summed probability. This is called soft voting and is preferred when the base-models used in the ensemble natively support predicting class probabilities as it can result in better performance.\n",
    "\n",
    "Voting ensembles are available in scikit-learn via the [VotingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) and [VotingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html) classes. A list of base-models can be provided as an argument to the model and each model in the list must be a tuple with a name and the model, e.g. (‘*lr’, LogisticRegression()*). The type of voting used for classification can be specified via the “*voting*” argument and set to either ‘*soft*‘ or ‘*hard*‘.\n",
    "\n",
    "The complete example of evaluating a voting ensemble for classification is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b93204ac-7541-4a65-b8e9-f2a36fa002d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.960 (0.061)\n"
     ]
    }
   ],
   "source": [
    "# example of evaluating a voting ensemble for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# create the synthetic classification dataset\n",
    "X, y = make_classification(random_state=1)\n",
    "# configure the models to use in the ensemble\n",
    "models = [('lr', LogisticRegression()), ('nb', GaussianNB())]\n",
    "# configure the ensemble model\n",
    "model = VotingClassifier(models, voting='soft')\n",
    "# configure the resampling method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the ensemble on the dataset using the resampling method\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report ensemble performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69671aa4-3a48-49af-a451-71f5926d7dd9",
   "metadata": {},
   "source": [
    "## Lesson 07: Stacking Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890d762d-4ea7-410d-8226-ba6970b3d652",
   "metadata": {},
   "source": [
    "In this lesson, you will discover the stacked generalization or stacking ensemble.\n",
    "\n",
    "Stacking involves combining the predictions of multiple different types of base-models, much like voting.\n",
    "\n",
    "The important difference from voting is that another machine learning model is used to learn how to best combine the predictions of the base-models. This is often a linear model, such as a linear regression for regression problems or logistic regression for classification, but can be any machine learning model you like.\n",
    "\n",
    "This involves using k-fold cross-validation for each base-model and storing all of the out-of-fold predictions. The base-models are then trained on the entire training dataset, and the meta-model is trained on the out-of-fold predictions and learns which model to trust, the degree to trust them, and under which circumstances.\n",
    "\n",
    "Although internally stacking uses k-fold cross-validation to train the meta model, you can evaluate stacking models any way you like, such as via a train-test split or k-fold cross-validation. The evaluation of the model is separate from this internal resampling-for-training process.\n",
    "\n",
    "Stacking ensembles are available in scikit-learn via the [StackingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html) and [StackingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html) classes. A list of base-models can be provided as an argument to the model and each model in the list must be a tuple with a name and the model, e.g. (‘*lr’, LogisticRegression()*). The meta-learner can be specified via the “*final_estimator*” argument and the resampling strategy can be specified via the “*cv*” argument and can be simply set to an integer indicating the number of cross-validation folds.\n",
    "\n",
    "The complete example of evaluating a stacking ensemble for classification is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b6caca-4f9a-4a94-b25c-7d7e9a1aa666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of evaluating a stacking ensemble for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# create the synthetic classification dataset\n",
    "X, y = make_classification(random_state=1)\n",
    "# configure the models to use in the ensemble\n",
    "models = [('knn', KNeighborsClassifier()), ('tree', DecisionTreeClassifier())]\n",
    "# configure the ensemble model\n",
    "model = StackingClassifier(models, final_estimator=LogisticRegression(), cv=3)\n",
    "# configure the resampling method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the ensemble on the dataset using the resampling method\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report ensemble performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6672a3f3-f299-4f2a-a838-b3ab48ac296d",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb8c93a-0ff1-474e-b30d-a436ed198885",
   "metadata": {},
   "source": [
    "Take a moment and look back at how far you have come.\n",
    "\n",
    "You discovered:\n",
    "\n",
    "- What ensemble learning is and why you would use it on a predictive modeling project.\n",
    "- How to use a bootstrap aggregation, or bagging, ensemble.\n",
    "- How to use a random forest ensemble as an extension to bagging.\n",
    "- How to use an adaptive boosting or adaboost ensemble.\n",
    "- How to use a gradient boosting ensemble.\n",
    "- How to combine the predictions of models using a voting ensemble.\n",
    "- How to learn how to combine the predictions of models using a stacking ensemble."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
